<workflow-app xmlns="uri:oozie:workflow:0.2" name="user-basic-wf">
    <start to="hive-node"/>
    <!--<action name="a" retry-max="3" retry-interval="1">-->
    <action name="hive-node">
        <hive xmlns="uri:oozie:hive-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
            </configuration>
            <script>customer.sql</script>
            <param>partition=${partition}</param>
        </hive>
        <ok to="mr-node"/>
        <error to="fail"/>
    </action>
    <action name="mr-node">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.map.class</name>
                    <value>com.lianjia.profiling.batch.Customer$CustomerMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.TextInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>org.elasticsearch.hadoop.mr.EsOutputFormat</value>
                </property>
                <property>
                    <name>mapreduce.map.output.key.class</name>
                    <value>org.apache.hadoop.io.NullWritable</value>
                </property>
                <property>
                    <name>mapreduce.map.output.value.class</name>
                    <value>org.apache.hadoop.io.MapWritable</value>
                </property>
                <!-- todo 测下mapreduce.job.reduces -->
                <property>
                    <name>mapreduce.job.reduces</name>
                    <value>20</value>
                </property>
                <property>
                    <name>mapred.input.dir</name>
                    <value>/user/hive/warehouse/profiling.db/customer/0*</value>
                </property>
                <property>
                    <name>es.nodes</name>
                    <value>172.30.17.1:9200,172.30.17.2:9200</value>
                </property>
                <property>
                    <name>es.resource</name>
                    <value>customer/customer</value>
                </property>
                <property>
                    <name>es.mapping.id</name>
                    <value>cust_id</value>
                </property>
                <property>
                    <name>mapred.map.tasks.speculative.execution</name>
                    <value>false</value>
                </property>
                <property>
                    <name>mapred.reduce.tasks.speculative.execution</name>
                    <value>false</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="end"/>
        <error to="fail"/>
    </action>
    <kill name="fail">
        <message>customer m/r failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name="end"/>
</workflow-app>
